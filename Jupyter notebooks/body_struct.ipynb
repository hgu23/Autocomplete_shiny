{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import pickle\n",
    "import autocomplete2\n",
    "from collections import Counter\n",
    "from bottle import route, run, debug\n",
    "from autocomplete2 import predict\n",
    "from autocomplete2 import models\n",
    "from autocomplete2 import helpers\n",
    "from autocomplete2 import split_predict\n",
    "from autocomplete2 import predict_currword\n",
    "from autocomplete2 import predict_second_word_given_first\n",
    "from autocomplete2 import predict_currword_given_lastword\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the body structures MIMIC-III database ###\n",
    "body_struct = open(\"body_structure_100k.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9102582\n"
     ]
    }
   ],
   "source": [
    "print(len(body_struct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"['serosal', 'mass', 'involvin\n"
     ]
    }
   ],
   "source": [
    "print(body_struct[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Splitting the one long string into a list of queries using the chosen split character ####\n",
    "splitted = body_struct.split(']\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100001\n"
     ]
    }
   ],
   "source": [
    "print(len(splitted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"['ct', 'pelvis', 'with', 'intravenous', 'contrast', ':', 'a', 'heterogeneous', 'centrally'\n"
     ]
    }
   ],
   "source": [
    "print(splitted[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Removing unwanted characters from each query and appending it into a list ####\n",
    "string_list = []\n",
    "for i in range(0,len(splitted)):\n",
    "    string = splitted[i]\n",
    "    new = string.replace('\"[','')\n",
    "    new2 = new.replace(\"'\",'')\n",
    "    string_list.append(new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the, colon, is, displaced, in, multiple, other, locations, by\n"
     ]
    }
   ],
   "source": [
    "print(string_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Further splitting each query list using , as the split character ####\n",
    "word_list = []\n",
    "for i in range(0,len(string_list)):\n",
    "    string = string_list[i]\n",
    "    new = string.split(\", \")\n",
    "    word_list.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nthe', 'colon', 'is', 'displaced', 'in', 'multiple', 'other', 'locations', 'by']\n"
     ]
    }
   ],
   "source": [
    "print(word_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Converting each query from a list of strings to a sentence ####\n",
    "query_list = []\n",
    "for k in range(0,len(word_list)):\n",
    "    curr_word_list = word_list[k]\n",
    "    query = ' '.join(word for word in curr_word_list)\n",
    "    query_list.append(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tethering the terminal ileum and cecum as well as the sigmoid\n"
     ]
    }
   ],
   "source": [
    "print(query_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Writing each sentence into a new text file ####\n",
    "with open('/Users/harshitg/github/autocomplete/autocomplete/body_struct.txt','w') as f:\n",
    "    for item in query_list:\n",
    "        f.write(\"%s\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Opening the newly constructed .txt file ####\n",
    "txt = open(\"body_struct.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to: /Users/harshitg/github/autocomplete/autocomplete/models_compressed.pkl\n",
      "saving to: /Users/harshitg/github/autocomplete/autocomplete/models_compressed.pkl\n",
      "saving to: /Users/harshitg/github/autocomplete/autocomplete/models_compressed.pkl\n",
      "saving to: /Users/harshitg/github/autocomplete/autocomplete/models_compressed.pkl\n",
      "saving to: /Users/harshitg/github/autocomplete/autocomplete/models_compressed.pkl\n",
      "saving to: /Users/harshitg/github/autocomplete/autocomplete/models_compressed.pkl\n",
      "saving to: /Users/harshitg/github/autocomplete/autocomplete/models_compressed.pkl\n",
      "saving to: /Users/harshitg/github/autocomplete/autocomplete/models_compressed.pkl\n",
      "saving to: /Users/harshitg/github/autocomplete/autocomplete/models_compressed.pkl\n",
      "saving to: /Users/harshitg/github/autocomplete/autocomplete/models_compressed.pkl\n",
      "[2.8892760276794434, 2.6587746143341064, 2.7263906002044678, 2.6975269317626953, 2.837463140487671, 2.6083760261535645, 2.7279722690582275, 2.7710158824920654, 3.496490955352783, 3.2298569679260254]\n"
     ]
    }
   ],
   "source": [
    "##### Training python's autocomplete on the pre-processed .txt file #####\n",
    "k = 0\n",
    "elapsed_list = []\n",
    "while k<10:\n",
    "    start = time.time()\n",
    "    models.train_models(txt,model_name=\"models_compressed.pkl\")\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start \n",
    "    elapsed_list.append(elapsed_time)\n",
    "    k += 1\n",
    "print(elapsed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('failure', 3020),\n",
       " ('rate', 1689),\n",
       " ('disease', 353),\n",
       " ('sounds', 348),\n",
       " ('size', 278),\n",
       " ('is', 232),\n",
       " ('was', 195),\n",
       " ('block', 192),\n",
       " ('attack', 192),\n",
       " ('and', 102)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Predicting the second word given only the first word #####\n",
    "predict_second_word_given_first(\"heart\",top_n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number following each autocomplete suggestion is the frequency of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('heart', 8035),\n",
       " ('head', 3884),\n",
       " ('his', 3264),\n",
       " ('had', 2756),\n",
       " ('her', 2749),\n",
       " ('he', 2189),\n",
       " ('history', 1700),\n",
       " ('have', 1476),\n",
       " ('hip', 1395),\n",
       " ('has', 1243)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### Predicting only the current word #####\n",
    "predict_currword(\"h\",top_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('attack', 192), ('and', 102), ('attacks', 13), ('arrhythmia', 5), ('at', 4)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Predicting the second word given the first word and part of the second word ####\n",
    "predict_currword_given_lastword(\"heart\",\"a\",top_n = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
